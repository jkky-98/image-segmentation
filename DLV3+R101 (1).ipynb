{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 18466,
     "status": "ok",
     "timestamp": 1690309017400,
     "user": {
      "displayName": "aj thos",
      "userId": "00936775760953736759"
     },
     "user_tz": -540
    },
    "id": "mGd5LXnQQuMJ"
   },
   "outputs": [],
   "source": [
    "##################libary import####################\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('C:/Users/JKKY/Desktop/project_dacon/deeplabv3+/dacon/segmentation_models_pytorch')\n",
    "# 본인 segmentation_models_pytorch 폴더의 경로를 입력하세요\n",
    "import segmentation_models_pytorch as smp\n",
    "import segmentation_models_pytorch.utils as smpu\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Subset\n",
    "# 위에 경로 잘 설정 부탁드립니다 ㅎㅎ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6XFQCo6vRT-l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변경된 작업 경로: C:\\Users\\JKKY\\Desktop\\project_dacon\\deeplabv3+\\dacon\n"
     ]
    }
   ],
   "source": [
    "new_path = \"C:/Users/JKKY/Desktop/project_dacon/deeplabv3+/dacon\"\n",
    "\n",
    "# 새로운 작업 경로로 변경\n",
    "os.chdir(new_path)\n",
    "\n",
    "# 변경된 작업 경로 출력\n",
    "current_path = os.getcwd()\n",
    "print(\"변경된 작업 경로:\", current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지의 shape: (224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# 이미지 파일 경로를 지정합니다.\n",
    "image_path = \"C:/Users/JKKY/Desktop/project_dacon/deeplabv3+/dacon/train_img_16crop/image_crop_1.png\"\n",
    "\n",
    "# 이미지를 로드합니다.\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# 이미지를 NumPy 배열로 변환합니다.\n",
    "image_array = np.array(image)\n",
    "\n",
    "# 이미지의 shape을 확인합니다. (높이, 너비, 채널(RGB) 또는 채널 없음(흑백)으로 표시)\n",
    "print(\"이미지의 shape:\", image_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Qe4LyygyQ_z-"
   },
   "outputs": [],
   "source": [
    "##################module setting#####################\n",
    "\n",
    "# RLE 디코딩 함수\n",
    "def rle_decode(mask_rle, shape):\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip,\n",
    "    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout,\n",
    "    IAAAdditiveGaussianNoise, Transpose, Lambda\n",
    "    )\n",
    "def get_training_augmentation(width=320, height=320):\n",
    "    train_transform = [\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "        A.PadIfNeeded(min_height=height, min_width=width, always_apply=True, border_mode=0),\n",
    "        A.RandomCrop(height=height, width=width, always_apply=True),\n",
    "        A.GaussNoise(p=0.2),\n",
    "        A.Perspective(p=0.5),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.CLAHE(p=1),\n",
    "                A.RandomBrightnessContrast(p=1),\n",
    "                A.RandomGamma(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.Sharpen(p=1),\n",
    "                A.Blur(blur_limit=3, p=1),\n",
    "                A.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.RandomBrightnessContrast(p=1),\n",
    "                A.HueSaturationValue(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "        # A.Normalize(mean=[87.24029665, 91.22533398, 82.92776534],\n",
    "        #             std=[49.33672613, 44.09863433, 42.24505498])\n",
    "    ]\n",
    "    return A.Compose(train_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    _transform = [\n",
    "        Lambda(image=preprocessing_fn), # preprocessing with encoder contained encoder weights\n",
    "        Lambda(image=to_tensor, mask=to_tensor) # ability to tensor\n",
    "    ]\n",
    "    return Compose(_transform)\n",
    "\n",
    "class SatelliteDatasetInfer(Dataset):\n",
    "    def __init__(self, csv_file, transforms=None, preprocessing=None, infer=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transforms = transforms\n",
    "        self.preprocessing = preprocessing\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx, 1]\n",
    "        img = Image.open(img_path).convert('RGB') # PIL Image (not numpy dtype)\n",
    "\n",
    "        if self.infer: # in infer mode, only give image\n",
    "            if self.transforms:\n",
    "              img = self.transforms(image=np.array(img))['image']\n",
    "            if self.preprocessing:\n",
    "              img = self.preprocessing(image=img)['image']\n",
    "            return img\n",
    "\n",
    "        mask_rle = self.data.iloc[idx, 2]\n",
    "        mask = rle_decode(mask_rle, (1024, 1024, 1)) # shape : (1024, 1024, 1), numpy dtype\n",
    "\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=np.array(img), mask=mask) # transforming : plz insert input(numpy dtype)\n",
    "            img = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        if self.preprocessing:\n",
    "            augmented = self.preprocessing(image=img, mask=mask)\n",
    "            img = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return img, mask #\n",
    "\n",
    "def numerical_sort(value):\n",
    "    # 정렬 시, 파일 이름의 숫자 부분을 기준으로 정렬하기 위해 사용\n",
    "    numbers = re.compile(r'(\\d+)')\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts\n",
    "\n",
    "class CropDataset(Dataset):\n",
    "    def __init__(self, img_path, mask_path, transforms=None, preprocessing=None, infer=False):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.img_lst = []\n",
    "        self.mask_lst = []\n",
    "        self.transforms = transforms\n",
    "        self.preprocessing = preprocessing\n",
    "        self.infer = infer\n",
    "        self.sort_by_numerical()\n",
    "\n",
    "    def sort_by_numerical(self):\n",
    "        self.img_lst = os.listdir(self.img_path)\n",
    "        self.img_lst.sort(key=self.numerical_sort)\n",
    "        self.mask_lst = os.listdir(self.mask_path)\n",
    "        self.mask_lst.sort(key=self.numerical_sort)\n",
    "\n",
    "    def numerical_sort(self, value):\n",
    "        # 정렬 시, 파일 이름의 숫자 부분을 기준으로 정렬하기 위해 사용\n",
    "        numbers = re.compile(r'(\\d+)')\n",
    "        parts = numbers.split(value)\n",
    "        parts[1::2] = map(int, parts[1::2])\n",
    "        return parts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_lst)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_pth = self.img_path +'/'+ self.img_lst[idx]\n",
    "        mask_pth = self.mask_path +'/'+ self.mask_lst[idx]\n",
    "\n",
    "        img = Image.open(img_pth).convert('RGB') # PIL Image (not numpy dtype)\n",
    "\n",
    "        if self.infer: # in infer mode, only give image\n",
    "            if self.transforms:\n",
    "                img = self.transforms(image=np.array(img))['image']\n",
    "            if self.preprocessing:\n",
    "                img = self.preprocessing(image=img)['image']\n",
    "            return img\n",
    "        mask = cv2.imread(mask_pth, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = np.reshape(mask, (224, 224, 1))\n",
    "        mask = mask / 255\n",
    "         # shape : (1024, 1024, 1), numpy dtype\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=np.array(img), mask=mask) # transforming : plz insert input(numpy dtype)\n",
    "            img = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        if self.preprocessing:\n",
    "            augmented = self.preprocessing(image=img, mask=mask)\n",
    "            img = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "def split_dataset(dataset, split_ratio=0.8, random_seed=None):\n",
    "    \"\"\"\n",
    "    데이터셋을 훈련과 검증 데이터셋으로 나누는 함수\n",
    "\n",
    "    Args:\n",
    "        dataset (torch.utils.data.Dataset): 전체 데이터셋\n",
    "        split_ratio (float): 훈련 데이터셋의 비율 (0과 1 사이의 값)\n",
    "        random_seed (int): 랜덤 시드 값 (기본값: None)\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.data.Dataset: 훈련 데이터셋\n",
    "        torch.utils.data.Dataset: 검증 데이터셋\n",
    "    \"\"\"\n",
    "\n",
    "    # 데이터셋 크기를 구하고 셔플합니다.\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    # 지정한 비율로 데이터셋을 나눕니다.\n",
    "    split = int(split_ratio * dataset_size)\n",
    "    train_indices, val_indices = indices[:split], indices[split:]\n",
    "\n",
    "    # Subset으로 나눈 후 반환합니다.\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "def get_test_augmentation(width=224, height=224):\n",
    "    train_transform = [\n",
    "        A.GaussNoise(p=0),\n",
    "        # A.OneOf(\n",
    "        #     [\n",
    "        #         A.CLAHE(p=1),\n",
    "        #         A.RandomBrightnessContrast(p=1),\n",
    "        #         A.RandomGamma(p=1),\n",
    "        #     ],\n",
    "        #     p=0.9,\n",
    "        # ),\n",
    "\n",
    "        # A.OneOf(\n",
    "        #     [\n",
    "        #         A.Sharpen(p=1),\n",
    "        #         A.Blur(blur_limit=3, p=1),\n",
    "        #         A.MotionBlur(blur_limit=3, p=1),\n",
    "        #     ],\n",
    "        #     p=0.9,\n",
    "        # ),\n",
    "\n",
    "        # A.OneOf(\n",
    "        #     [\n",
    "        #         A.RandomBrightnessContrast(p=1),\n",
    "        #         A.HueSaturationValue(p=1),\n",
    "        #     ],\n",
    "        #     p=0.9,\n",
    "        # ),\n",
    "        # A.Normalize(mean=[87.24029665, 91.22533398, 82.92776534],\n",
    "        #             std=[49.33672613, 44.09863433, 42.24505498])\n",
    "    ]\n",
    "    return A.Compose(train_transform)\n",
    "\n",
    "def iscuda():\n",
    "    if torch.cuda.is_available():\n",
    "        # if colab notebook can do in CUDA\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"You have a GPU with CUDA enabled.\")\n",
    "        print(\"GPU in use:\", torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        # if colab notebook can't do in CUDA\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Cuda currently cannot use GPU, I will use CPU\")\n",
    "\n",
    "def set_workspace(path):\n",
    "  desired_directory = path\n",
    "  os.chdir(desired_directory)\n",
    "  current_directory = os.getcwd()\n",
    "  print(\"Changed work path :\", current_directory)\n",
    "\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "seed_torch()\n",
    "# 실행만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1690218877707,
     "user": {
      "displayName": "aj thos",
      "userId": "00936775760953736759"
     },
     "user_tz": -540
    },
    "id": "iAOiAE7gRBaw",
    "outputId": "71cc5676-3ff0-43d2-d817-ede87dab2ff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have a GPU with CUDA enabled.\n",
      "GPU in use: NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "####################현재 GPU설정 되었는지 확인###################\n",
    "iscuda()\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6rm_TYuKhpPQ"
   },
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "# SSL 인증서 검증 비활성화\n",
    "ssl._create_default_https_context = ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "ENCODER = 'resnet101' #fix\n",
    "ENCODER_WEIGHTS = 'imagenet' # fix\n",
    "CLASSES = ['building'] # fix\n",
    "ACTIVATION = 'sigmoid' # fix\n",
    "# create segmentation model with pretrained encoder\n",
    "\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=ENCODER,\n",
    "    encoder_weights=ENCODER_WEIGHTS,\n",
    "    classes=len(CLASSES),\n",
    "    activation=ACTIVATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "gJk1fd8Yifxc"
   },
   "outputs": [],
   "source": [
    "# encoder setting with encoder weights\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "_preprocessing = get_preprocessing(preprocessing_fn)\n",
    "\n",
    "# base train dataset generate\n",
    "dataset = CropDataset(img_path = 'C:/Users/JKKY/Desktop/project_dacon/deeplabv3+/dacon/train_img_16crop',\n",
    "                      mask_path = 'C:/Users/JKKY/Desktop/project_dacon/deeplabv3+/dacon/mask_img_16crop',\n",
    "                     transforms=get_training_augmentation(256,256),\n",
    "                     preprocessing=_preprocessing,\n",
    "                                     infer=False)\n",
    "# sampler generate\n",
    "train_dataset, val_dataset = split_dataset(dataset, 0.9, 42)\n",
    "\n",
    "# train % validation DataLoader generate\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, num_workers=0, pin_memory=True, drop_last=True, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, num_workers=0, pin_memory=True, drop_last=True, shuffle=False)\n",
    "\n",
    "# base test dataset genrate\n",
    "test_dataset = SatelliteDatasetInfer(csv_file='./test.csv',\n",
    "                                     transforms=get_training_augmentation(224, 224)\n",
    "                                     ,preprocessing=_preprocessing\n",
    "                                     ,infer=True)\n",
    "\n",
    "# test DataLoader generate\n",
    "batch_size_test = 32\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size_test, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.9295, -1.9295, -1.9295,  ..., -1.9295, -1.9295, -1.9295],\n",
      "          [-1.9295, -1.9295, -1.9295,  ..., -1.9295, -1.9295, -1.9295],\n",
      "          [-1.9295, -1.9295, -1.9295,  ..., -1.9295, -1.9295, -1.9295],\n",
      "          ...,\n",
      "          [-1.9295, -1.9295, -1.9295,  ..., -1.9295, -1.9295, -1.9295],\n",
      "          [-1.9295, -1.9295, -1.9295,  ..., -1.9295, -1.9295, -1.9295],\n",
      "          [-1.9295, -1.9295, -1.9295,  ..., -1.9295, -1.9295, -1.9295]],\n",
      "\n",
      "         [[-1.8431, -1.8431, -1.8431,  ..., -1.8431, -1.8431, -1.8431],\n",
      "          [-1.8431, -1.8431, -1.8431,  ..., -1.8431, -1.8431, -1.8431],\n",
      "          [-1.8431, -1.8431, -1.8431,  ..., -1.8431, -1.8431, -1.8431],\n",
      "          ...,\n",
      "          [-1.8431, -1.8431, -1.8431,  ..., -1.8431, -1.8431, -1.8431],\n",
      "          [-1.8431, -1.8431, -1.8431,  ..., -1.8431, -1.8431, -1.8431],\n",
      "          [-1.8431, -1.8431, -1.8431,  ..., -1.8431, -1.8431, -1.8431]],\n",
      "\n",
      "         [[-1.6127, -1.6127, -1.6127,  ..., -1.6127, -1.6127, -1.6127],\n",
      "          [-1.6127, -1.6127, -1.6127,  ..., -1.6127, -1.6127, -1.6127],\n",
      "          [-1.6127, -1.6127, -1.6127,  ..., -1.6127, -1.6127, -1.6127],\n",
      "          ...,\n",
      "          [-1.6127, -1.6127, -1.6127,  ..., -1.6127, -1.6127, -1.6127],\n",
      "          [-1.6127, -1.6127, -1.6127,  ..., -1.6127, -1.6127, -1.6127],\n",
      "          [-1.6127, -1.6127, -1.6127,  ..., -1.6127, -1.6127, -1.6127]]],\n",
      "\n",
      "\n",
      "        [[[-1.5870, -1.5870, -1.5870,  ..., -1.2788, -1.3130, -1.3130],\n",
      "          [-1.5870, -1.5870, -1.5870,  ..., -1.2959, -1.3302, -1.3302],\n",
      "          [-1.5870, -1.5870, -1.5870,  ..., -1.3302, -1.3644, -1.3644],\n",
      "          ...,\n",
      "          [-1.5870, -1.5870, -1.5870,  ..., -1.5870, -1.5870, -1.5870],\n",
      "          [-1.5870, -1.5870, -1.5870,  ..., -1.5870, -1.5870, -1.5870],\n",
      "          [-1.5870, -1.5870, -1.5870,  ..., -1.5870, -1.5870, -1.5870]],\n",
      "\n",
      "         [[-1.4930, -1.4930, -1.4930,  ..., -0.9153, -0.9503, -0.9503],\n",
      "          [-1.4930, -1.4930, -1.4930,  ..., -0.9328, -0.9678, -0.9678],\n",
      "          [-1.4930, -1.4930, -1.4930,  ..., -0.9328, -0.9853, -0.9853],\n",
      "          ...,\n",
      "          [-1.4930, -1.4930, -1.4930,  ..., -1.4930, -1.4930, -1.4930],\n",
      "          [-1.4930, -1.4930, -1.4930,  ..., -1.4930, -1.4930, -1.4930],\n",
      "          [-1.4930, -1.4930, -1.4930,  ..., -1.4930, -1.4930, -1.4930]],\n",
      "\n",
      "         [[-1.2641, -1.2641, -1.2641,  ..., -0.4624, -0.4624, -0.4624],\n",
      "          [-1.2641, -1.2641, -1.2641,  ..., -0.4624, -0.4798, -0.4973],\n",
      "          [-1.2641, -1.2641, -1.2641,  ..., -0.4798, -0.5495, -0.5495],\n",
      "          ...,\n",
      "          [-1.2641, -1.2641, -1.2641,  ..., -1.2641, -1.2641, -1.2641],\n",
      "          [-1.2641, -1.2641, -1.2641,  ..., -1.2641, -1.2641, -1.2641],\n",
      "          [-1.2641, -1.2641, -1.2641,  ..., -1.2641, -1.2641, -1.2641]]],\n",
      "\n",
      "\n",
      "        [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0182, -2.0182, -2.0182],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.7522, -1.7522, -1.7522],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.9638, -1.9638, -1.9638,  ..., -1.9638, -1.9638, -1.9638],\n",
      "          [-1.9638, -1.9638, -1.9638,  ..., -1.9638, -1.9638, -1.9638],\n",
      "          [-1.9638, -1.9638, -1.9638,  ..., -1.9638, -1.9638, -1.9638],\n",
      "          ...,\n",
      "          [-1.9638, -1.9638, -1.9638,  ..., -1.9638, -1.9638, -1.9638],\n",
      "          [-1.9638, -1.9638, -1.9638,  ..., -1.9638, -1.9638, -1.9638],\n",
      "          [-1.9638, -1.9638, -1.9638,  ..., -1.9638, -1.9638, -1.9638]],\n",
      "\n",
      "         [[-1.8782, -1.8782, -1.8782,  ..., -1.8782, -1.8782, -1.8782],\n",
      "          [-1.8782, -1.8782, -1.8782,  ..., -1.8782, -1.8782, -1.8782],\n",
      "          [-1.8782, -1.8782, -1.8782,  ..., -1.8782, -1.8782, -1.8782],\n",
      "          ...,\n",
      "          [-1.8782, -1.8782, -1.8782,  ..., -1.8782, -1.8782, -1.8782],\n",
      "          [-1.8782, -1.8782, -1.8782,  ..., -1.8782, -1.8782, -1.8782],\n",
      "          [-1.8782, -1.8782, -1.8782,  ..., -1.8782, -1.8782, -1.8782]],\n",
      "\n",
      "         [[-1.6476, -1.6476, -1.6476,  ..., -1.6476, -1.6476, -1.6476],\n",
      "          [-1.6476, -1.6476, -1.6476,  ..., -1.6476, -1.6476, -1.6476],\n",
      "          [-1.6476, -1.6476, -1.6476,  ..., -1.6476, -1.6476, -1.6476],\n",
      "          ...,\n",
      "          [-1.6476, -1.6476, -1.6476,  ..., -1.6476, -1.6476, -1.6476],\n",
      "          [-1.6476, -1.6476, -1.6476,  ..., -1.6476, -1.6476, -1.6476],\n",
      "          [-1.6476, -1.6476, -1.6476,  ..., -1.6476, -1.6476, -1.6476]]],\n",
      "\n",
      "\n",
      "        [[[-1.6898, -1.6898, -1.6898,  ..., -1.6898, -1.6898, -1.6898],\n",
      "          [-1.6898, -1.6898, -1.6898,  ..., -1.6898, -1.6898, -1.6898],\n",
      "          [-1.6898, -1.6898, -1.6898,  ..., -1.6898, -1.6898, -1.6898],\n",
      "          ...,\n",
      "          [-1.6898, -1.6898, -1.6898,  ..., -1.6898, -1.6898, -1.6898],\n",
      "          [-1.6898, -1.6898, -1.6898,  ..., -1.6898, -1.6898, -1.6898],\n",
      "          [-1.6898, -1.6898, -1.6898,  ..., -1.6898, -1.6898, -1.6898]],\n",
      "\n",
      "         [[-1.5980, -1.5980, -1.5980,  ..., -1.5980, -1.5980, -1.5980],\n",
      "          [-1.5980, -1.5980, -1.5980,  ..., -1.5980, -1.5980, -1.5980],\n",
      "          [-1.5980, -1.5980, -1.5980,  ..., -1.5980, -1.5980, -1.5980],\n",
      "          ...,\n",
      "          [-1.5980, -1.5980, -1.5980,  ..., -1.5980, -1.5980, -1.5980],\n",
      "          [-1.5980, -1.5980, -1.5980,  ..., -1.5980, -1.5980, -1.5980],\n",
      "          [-1.5980, -1.5980, -1.5980,  ..., -1.5980, -1.5980, -1.5980]],\n",
      "\n",
      "         [[-1.3687, -1.3687, -1.3687,  ..., -1.3687, -1.3687, -1.3687],\n",
      "          [-1.3687, -1.3687, -1.3687,  ..., -1.3687, -1.3687, -1.3687],\n",
      "          [-1.3687, -1.3687, -1.3687,  ..., -1.3687, -1.3687, -1.3687],\n",
      "          ...,\n",
      "          [-1.3687, -1.3687, -1.3687,  ..., -1.3687, -1.3687, -1.3687],\n",
      "          [-1.3687, -1.3687, -1.3687,  ..., -1.3687, -1.3687, -1.3687],\n",
      "          [-1.3687, -1.3687, -1.3687,  ..., -1.3687, -1.3687, -1.3687]]],\n",
      "\n",
      "\n",
      "        [[[-1.8268, -1.8268, -1.8268,  ..., -1.8268, -1.8268, -1.8268],\n",
      "          [-1.8268, -1.8268, -1.8268,  ..., -1.8268, -1.8268, -1.8268],\n",
      "          [-1.8268, -1.8268, -1.8268,  ..., -1.8268, -1.8268, -1.8268],\n",
      "          ...,\n",
      "          [-1.8268, -1.8268, -1.8268,  ..., -1.8268, -1.8268, -1.8268],\n",
      "          [-1.8268, -1.8268, -1.8268,  ..., -1.8268, -1.8268, -1.8268],\n",
      "          [-1.8268, -1.8268, -1.8268,  ..., -1.8268, -1.8268, -1.8268]],\n",
      "\n",
      "         [[-1.7381, -1.7381, -1.7381,  ..., -1.7381, -1.7381, -1.7381],\n",
      "          [-1.7381, -1.7381, -1.7381,  ..., -1.7381, -1.7381, -1.7381],\n",
      "          [-1.7381, -1.7381, -1.7381,  ..., -1.7381, -1.7381, -1.7381],\n",
      "          ...,\n",
      "          [-1.7381, -1.7381, -1.7381,  ..., -1.7381, -1.7381, -1.7381],\n",
      "          [-1.7381, -1.7381, -1.7381,  ..., -1.7381, -1.7381, -1.7381],\n",
      "          [-1.7381, -1.7381, -1.7381,  ..., -1.7381, -1.7381, -1.7381]],\n",
      "\n",
      "         [[-1.5081, -1.5081, -1.5081,  ..., -1.5081, -1.5081, -1.5081],\n",
      "          [-1.5081, -1.5081, -1.5081,  ..., -1.5081, -1.5081, -1.5081],\n",
      "          [-1.5081, -1.5081, -1.5081,  ..., -1.5081, -1.5081, -1.5081],\n",
      "          ...,\n",
      "          [-1.5081, -1.5081, -1.5081,  ..., -1.5081, -1.5081, -1.5081],\n",
      "          [-1.5081, -1.5081, -1.5081,  ..., -1.5081, -1.5081, -1.5081],\n",
      "          [-1.5081, -1.5081, -1.5081,  ..., -1.5081, -1.5081, -1.5081]]]])\n",
      "tensor(13265.)\n"
     ]
    }
   ],
   "source": [
    "for i,m in train_loader:\n",
    "    print(i)\n",
    "    print(m.sum())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dwq9JyMYiVFJ"
   },
   "outputs": [],
   "source": [
    "loss = smp.utils.losses.DiceLoss()\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "]\n",
    "optimizer = torch.optim.Adam([\n",
    "    dict(params=model.parameters(), lr=0.0001),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aog2wVjXi_yd"
   },
   "outputs": [],
   "source": [
    "train_epoch = smpu.train.TrainEpoch(\n",
    "    model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smpu.train.ValidEpoch(\n",
    "    model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    device=device,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ux6uvVECj-XQ"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-_Z0AOVj0Mx",
    "outputId": "c93ac7bb-7ec1-42c9-a075-717fdc1d05fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "train:   0%|▏                             | 59/12852 [00:17<1:01:35,  3.46it/s, dice_loss - -0.6762, iou_score - 14.79]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m     train_logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     valid_logs \u001b[38;5;241m=\u001b[39m valid_epoch\u001b[38;5;241m.\u001b[39mrun(val_loader)\n\u001b[0;32m     17\u001b[0m     x_epoch_data\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "File \u001b[1;32m~\\Desktop\\project_dacon\\deeplabv3+\\dacon\\segmentation_models_pytorch\\utils\\train.py:54\u001b[0m, in \u001b[0;36mEpoch.run\u001b[1;34m(self, dataloader)\u001b[0m\n\u001b[0;32m     51\u001b[0m loss, y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_update(x, y)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# update loss logs\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     55\u001b[0m loss_meter\u001b[38;5;241m.\u001b[39madd(loss_value)\n\u001b[0;32m     56\u001b[0m loss_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m: loss_meter\u001b[38;5;241m.\u001b[39mmean}\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_score = 0\n",
    "\n",
    "\n",
    "#train accurascy, train loss, val_accuracy, val_loss\n",
    "x_epoch_data = []\n",
    "train_dice_loss = []\n",
    "train_iou_score = []\n",
    "valid_dice_loss = []\n",
    "valid_iou_score = []\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "\n",
    "    print(f'\\nEpoch: {i + 1}')\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(val_loader)\n",
    "\n",
    "    x_epoch_data.append(i)\n",
    "    train_dice_loss.append(train_logs['dice_loss'])\n",
    "    train_iou_score.append(train_logs['iou_score'])\n",
    "    valid_dice_loss.append(valid_logs['dice_loss'])\n",
    "    valid_iou_score.append(valid_logs['iou_score'])\n",
    "\n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score < valid_logs['iou_score']:\n",
    "        max_score = valid_logs['iou_score']\n",
    "        torch.save(model, './best_model_DLV3+_se_resnet101_2.pth')\n",
    "        print('Model saved!')\n",
    "\n",
    "    if i == 25:\n",
    "      optimizer.param_groups[0]['lr'] = 1e-5\n",
    "      print('Decrease decoder learning rate to 1e-5!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPjd6ZnGhbsXETbzoxa93p1",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
